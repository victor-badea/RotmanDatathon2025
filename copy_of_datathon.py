# -*- coding: utf-8 -*-
"""Copy of Datathon

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HDbYEVd6WPLr51gumq9X4QrlLYZw0vGT
"""

# Data manipulation and analysis
import pandas as pd  # pip install pandas
import numpy as np  # pip install numpy

# Data visualization
import matplotlib.pyplot as plt  # pip install matplotlib
import seaborn as sns  # pip install seaborn

# Machine learning
from sklearn.model_selection import train_test_split  # pip install scikit-learn
from sklearn.linear_model import LinearRegression  # pip install scikit-learn
from sklearn.metrics import mean_squared_error  # pip install scikit-learn
from sklearn.preprocessing import StandardScaler  # pip install scikit-learn
from sklearn.cluster import KMeans  # pip install scikit-learn
from sklearn.metrics import r2_score  # pip install scikit-learn
import xgboost as xgb  # pip install xgboost
import lightgbm as lgb  # pip install lightgbm

# Statistical analysis
from scipy.stats import ttest_ind  # pip install scipy
import statsmodels.api as sm  # pip install statsmodels

# Data processing
from dask import dataframe as dd  # pip install dask

# Utilities
from joblib import dump, load  # pip install joblib
from tqdm import tqdm  # pip install tqdm

from google.colab import drive
drive.mount('/content/drive')

trainData = pd.read_csv('/content/drive/MyDrive/imputed_data (1).csv')

trainData.sample(5)

keywords = ["cost", "supply", "economic", "income", "index", "GDP", "development"]
relevant_columns = [col for col in trainData.columns if any(keyword in col.lower() for keyword in keywords)]

relevant_data = trainData[relevant_columns]
relevant_data.head()

cost_of_living_cols = [
    "Consumer price index (2010 = 100) [FP.CPI.TOTL]",
    "Cost of business start-up procedures (% of GNI per capita) [IC.REG.COST.PC.ZS]",
    "Cost of business start-up procedures, female (% of GNI per capita) [IC.REG.COST.PC.FE.ZS]",
    "Cost of business start-up procedures, male (% of GNI per capita) [IC.REG.COST.PC.MA.ZS]"
]

economic_factors_cols = [
    "Adjusted net national income (current US$) [NY.ADJ.NNTY.CD]",
    "Adjusted net national income per capita (current US$) [NY.ADJ.NNTY.PC.CD]",
    "Adjusted net national income (annual % growth) [NY.ADJ.NNTY.KD.ZG]",
    "Income share held by highest 10% [SI.DST.10TH.10]",
    "Income share held by highest 20% [SI.DST.05TH.20]",
    "Income share held by lowest 10% [SI.DST.FRST.10]",
    "Income share held by lowest 20% [SI.DST.FRST.20]",
    "Income share held by second 20% [SI.DST.02ND.20]",
    "Income share held by third 20% [SI.DST.03RD.20]",
    "Income share held by fourth 20% [SI.DST.04TH.20]"
]

supply_chain_metrics_cols = [
    "Cost to export, border compliance (US$) [IC.EXP.CSBC.CD]",
    "Cost to import, border compliance (US$) [IC.IMP.CSBC.CD]",
    "Export unit value index (2015 = 100) [TX.UVI.MRCH.XD.WD]",
    "Short-term debt (% of exports of goods, services and primary income) [DT.DOD.DSTC.XP.ZS]"
]

cost_of_living_data = relevant_data[cost_of_living_cols]
economic_factors_data = relevant_data[economic_factors_cols]
supply_chain_metrics_data = relevant_data[supply_chain_metrics_cols]

def analyze_cost_impact():
    """
    Analyze how cost of living metrics impact supply chain costs
    """
    # Define target variables (supply chain costs)
    supply_cost_metrics = [
        "Cost to export, border compliance (US$) [IC.EXP.CSBC.CD]",
        "Cost to import, border compliance (US$) [IC.IMP.CSBC.CD]"
    ]

    # Prepare features (cost of living indicators)
    cost_features = [
        "Consumer price index (2010 = 100) [FP.CPI.TOTL]",
        "Cost of business start-up procedures (% of GNI per capita) [IC.REG.COST.PC.ZS]"
    ]

def analyze_cost_impact():
    """
    Analyze how cost of living metrics impact supply chain costs
    """
    # Define target variables (supply chain costs)
    supply_cost_metrics = [
        "Cost to export, border compliance (US$) [IC.EXP.CSBC.CD]",
        "Cost to import, border compliance (US$) [IC.IMP.CSBC.CD]"
    ]

    # Prepare features (cost of living indicators)
    cost_features = [
        "Consumer price index (2010 = 100) [FP.CPI.TOTL]",
        "Cost of business start-up procedures (% of GNI per capita) [IC.REG.COST.PC.ZS]"
    ]

    # Create correlation matrix
    correlation_data = trainData[cost_features + supply_cost_metrics]
    correlation_matrix = correlation_data.corr()

    # Prepare data for modeling
    X = trainData[cost_features].fillna(method='ffill')
    y = trainData[supply_cost_metrics].fillna(method='ffill')

    # Split and scale data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Train model
    model = xgb.XGBRegressor(random_state=42)
    model.fit(X_train_scaled, y_train)

    # Make predictions
    y_pred = model.predict(X_test_scaled)

    # Calculate impact scores
    feature_importance = pd.DataFrame({
        'Feature': cost_features,
        'Importance': model.feature_importances_
    }).sort_values('Importance', ascending=False)

    # Visualize correlations
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
    plt.title('Correlation between Cost of Living and Supply Chain Metrics')
    plt.tight_layout()

    # Create time series plot if time data is available
    if 'Year' in trainData.columns:
        plt.figure(figsize=(12, 6))
        for metric in supply_cost_metrics:
            plt.plot(trainData['Year'], trainData[metric], label=metric.split('[')[0])
        for cost in cost_features:
            plt.plot(trainData['Year'], trainData[cost], label=cost.split('[')[0], linestyle='--')
        plt.title('Cost of Living vs Supply Chain Costs Over Time')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()

    return {
        'correlation_matrix': correlation_matrix,
        'feature_importance': feature_importance,
        'model_performance': {
            'r2_score': r2_score(y_test, y_pred),
            'mse': mean_squared_error(y_test, y_pred)
        },
        'model': model,
        'test_data': (X_test, y_test, y_pred)
    }

def generate_insights(analysis_results):
    """
    Generate key insights from the analysis
    """
    insights = {
        'key_correlations': [],
        'impact_factors': [],
        'performance_metrics': {}
    }

    # Extract strongest correlations
    corr_matrix = analysis_results['correlation_matrix']
    for col in corr_matrix.columns:
        if 'Cost to' in col:
            correlations = corr_matrix[col].sort_values(ascending=False)
            insights['key_correlations'].append({
                'target': col,
                'strongest_correlation': {
                    'factor': correlations.index[1],
                    'value': correlations.iloc[1]
                }
            })

    # Extract top impact factors
    insights['impact_factors'] = analysis_results['feature_importance'].to_dict('records')

    # Add model performance metrics
    insights['performance_metrics'] = analysis_results['model_performance']

    return insights

analysis_results = analyze_cost_impact()
insights = generate_insights(analysis_results)

def analyze_economic_factors(trainData):
    """
    Analyze how economic factors impact supply chain costs
    """
    # Define supply chain cost metrics
    supply_chain_costs = [
        "Cost to export, border compliance (US$) [IC.EXP.CSBC.CD]",
        "Cost to import, border compliance (US$) [IC.IMP.CSBC.CD]"
    ]

    # Define economic factors
    economic_factors = [
        "Adjusted net national income (current US$) [NY.ADJ.NNTY.CD]",
        "Adjusted net national income per capita (current US$) [NY.ADJ.NNTY.PC.CD]",
        "Adjusted net national income (annual % growth) [NY.ADJ.NNTY.KD.ZG]",
        "Income share held by highest 10% [SI.DST.10TH.10]",
        "Income share held by lowest 20% [SI.DST.FRST.20]"
    ]

    # Create correlation matrix
    correlation_data = trainData[economic_factors + supply_chain_costs]
    correlation_matrix = correlation_data.corr()

    # Prepare data for modeling
    X = trainData[economic_factors].ffill().bfill()
    y = trainData[supply_chain_costs].ffill().bfill()

    # Split and scale data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Train model for export costs
    export_model = xgb.XGBRegressor(random_state=42)
    export_model.fit(X_train_scaled, y_train.iloc[:, 0])  # First column is export costs

    # Train model for import costs
    import_model = xgb.XGBRegressor(random_state=42)
    import_model.fit(X_train_scaled, y_train.iloc[:, 1])  # Second column is import costs

    # Calculate feature importance for both models
    export_importance = pd.DataFrame({
        'Feature': economic_factors,
        'Importance': export_model.feature_importances_
    }).sort_values('Importance', ascending=False)

    import_importance = pd.DataFrame({
        'Feature': economic_factors,
        'Importance': import_model.feature_importances_
    }).sort_values('Importance', ascending=False)

    # Visualize correlations
    plt.figure(figsize=(12, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
    plt.title('Correlation between Economic Factors and Supply Chain Costs')
    plt.tight_layout()

    # Create feature importance plots
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    # Export costs feature importance
    sns.barplot(data=export_importance, x='Importance', y='Feature', ax=ax1)
    ax1.set_title('Economic Factors Impact on Export Costs')

    # Import costs feature importance
    sns.barplot(data=import_importance, x='Importance', y='Feature', ax=ax2)
    ax2.set_title('Economic Factors Impact on Import Costs')

    plt.tight_layout()

    return {
        'correlation_matrix': correlation_matrix,
        'export_importance': export_importance,
        'import_importance': import_importance,
        'models': {
            'export': export_model,
            'import': import_model
        }
    }

def analyze_regional_variations(trainData):
    """
    Analyze regional variations in supply chain costs
    """
    if 'Region' in trainData.columns:
        # Calculate average costs by region
        regional_costs = trainData.groupby('Region')[
            ["Cost to export, border compliance (US$) [IC.EXP.CSBC.CD]",
             "Cost to import, border compliance (US$) [IC.IMP.CSBC.CD]"]
        ].mean()

        # Visualize regional variations
        plt.figure(figsize=(12, 6))
        regional_costs.plot(kind='bar')
        plt.title('Average Supply Chain Costs by Region')
        plt.xticks(rotation=45)
        plt.tight_layout()

        return regional_costs

    return None

def generate_economic_insights(analysis_results):
    """
    Generate insights about economic factors' impact on supply chain costs
    """
    insights = {
        'key_economic_drivers': [],
        'regional_patterns': [],
        'recommendations': []
    }

    # Extract top economic drivers
    for factor in analysis_results['export_importance'].head(3).itertuples():
        insights['key_economic_drivers'].append({
            'factor': factor.Feature,
            'impact_on_exports': factor.Importance
        })

    # Add correlations
    corr_matrix = analysis_results['correlation_matrix']
    export_correlations = corr_matrix["Cost to export, border compliance (US$) [IC.EXP.CSBC.CD]"].sort_values(ascending=False)
    import_correlations = corr_matrix["Cost to import, border compliance (US$) [IC.IMP.CSBC.CD]"].sort_values(ascending=False)

    insights['correlations'] = {
        'export': export_correlations,
        'import': import_correlations
    }

    return insights

# Run the analyses
economic_analysis = analyze_economic_factors(trainData)
regional_analysis = analyze_regional_variations(trainData)
insights = generate_economic_insights(economic_analysis)

# Print key findings
print("\nTop Economic Drivers of Supply Chain Costs:")
for driver in insights['key_economic_drivers']:
    print(f"\nFactor: {driver['factor'].split('[')[0]}")
    print(f"Impact on Export Costs: {driver['impact_on_exports']:.3f}")

print("\nStrongest Economic Correlations with Export Costs:")
for factor, corr in insights['correlations']['export'].items():
    if 'Cost to export' not in factor:
        print(f"{factor.split('[')[0]}: {corr:.3f}")
        if abs(corr) > 0.5:  # Only print strong correlations
            break

print("\nStrongest Economic Correlations with Import Costs:")
for factor, corr in insights['correlations']['import'].items():
    if 'Cost to import' not in factor:
        print(f"{factor.split('[')[0]}: {corr:.3f}")
        if abs(corr) > 0.5:  # Only print strong correlations
            break

def analyze_cost_mitigation_strategies(trainData):
    """
    Analyze potential strategies for mitigating supply chain costs
    """
    # Define key metrics
    cost_metrics = [
        "Cost to export, border compliance (US$) [IC.EXP.CSBC.CD]",
        "Cost to import, border compliance (US$) [IC.IMP.CSBC.CD]"
    ]

    economic_indicators = [
        "Adjusted net national income per capita (current US$) [NY.ADJ.NNTY.PC.CD]",
        "Adjusted net national income (annual % growth) [NY.ADJ.NNTY.KD.ZG]"
    ]

    # Prepare data
    analysis_data = trainData[cost_metrics + economic_indicators].copy()
    analysis_data = analysis_data.ffill().bfill()

    # Normalize data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(analysis_data)
    scaled_df = pd.DataFrame(scaled_data, columns=analysis_data.columns)

    # Identify cost patterns using clustering
    kmeans = KMeans(n_clusters=3, random_state=42)
    cost_clusters = kmeans.fit_predict(scaled_df[cost_metrics])

    # Analyze cost elasticity
    elasticity_model = LinearRegression()
    elasticity_model.fit(
        scaled_df[economic_indicators],
        scaled_df[cost_metrics]
    )

    # Calculate cost sensitivity
    cost_sensitivity = pd.DataFrame({
        'factor': economic_indicators,
        'export_sensitivity': elasticity_model.coef_[0],
        'import_sensitivity': elasticity_model.coef_[1]
    })

    # Identify optimal strategies based on clusters
    cluster_strategies = []
    for i in range(3):
        cluster_data = analysis_data[cost_clusters == i]
        cluster_strategies.append({
            'cluster': i,
            'avg_export_cost': cluster_data[cost_metrics[0]].mean(),
            'avg_import_cost': cluster_data[cost_metrics[1]].mean(),
            'size': len(cluster_data)
        })

    return {
        'cost_sensitivity': cost_sensitivity,
        'cluster_strategies': cluster_strategies,
        'clusters': cost_clusters,
        'elasticity_model': elasticity_model
    }

def generate_mitigation_strategies(analysis_results):
    """
    Generate specific strategies based on analysis results
    """
    strategies = {
        'cost_reduction': [],
        'efficiency_improvements': [],
        'risk_mitigation': []
    }

    # Cost reduction strategies based on sensitivity
    sensitivity = analysis_results['cost_sensitivity']
    for _, row in sensitivity.iterrows():
        if abs(row['export_sensitivity']) > abs(row['import_sensitivity']):
            strategies['cost_reduction'].append({
                'factor': row['factor'],
                'strategy': 'Focus on export cost optimization',
                'impact_score': abs(row['export_sensitivity'])
            })
        else:
            strategies['cost_reduction'].append({
                'factor': row['factor'],
                'strategy': 'Focus on import cost optimization',
                'impact_score': abs(row['import_sensitivity'])
            })

    # Efficiency improvements based on cluster analysis
    for cluster in analysis_results['cluster_strategies']:
        if cluster['avg_export_cost'] > cluster['avg_import_cost']:
            strategies['efficiency_improvements'].append({
                'cluster': cluster['cluster'],
                'strategy': 'Optimize export processes',
                'potential_saving': cluster['avg_export_cost'] - cluster['avg_import_cost']
            })
        else:
            strategies['efficiency_improvements'].append({
                'cluster': cluster['cluster'],
                'strategy': 'Optimize import processes',
                'potential_saving': cluster['avg_import_cost'] - cluster['avg_export_cost']
            })

    return strategies

def visualize_strategy_analysis(analysis_results, trainData):
    """
    Create visualizations for strategy analysis
    """
    # Cost sensitivity plot
    plt.figure(figsize=(10, 6))
    sensitivity_data = analysis_results['cost_sensitivity']

    x = np.arange(len(sensitivity_data))
    width = 0.35

    plt.bar(x - width/2, sensitivity_data['export_sensitivity'],
            width, label='Export Sensitivity')
    plt.bar(x + width/2, sensitivity_data['import_sensitivity'],
            width, label='Import Sensitivity')

    plt.xlabel('Economic Factors')
    plt.ylabel('Cost Sensitivity')
    plt.title('Supply Chain Cost Sensitivity to Economic Factors')
    plt.xticks(x, sensitivity_data['factor'], rotation=45)
    plt.legend()
    plt.tight_layout()

    # Cluster analysis plot
    plt.figure(figsize=(10, 6))
    for i, cluster in enumerate(analysis_results['cluster_strategies']):
        plt.scatter(cluster['avg_export_cost'],
                   cluster['avg_import_cost'],
                   s=cluster['size']*50,
                   label=f'Cluster {i}')

    plt.xlabel('Average Export Cost')
    plt.ylabel('Average Import Cost')
    plt.title('Supply Chain Cost Clusters')
    plt.legend()
    plt.tight_layout()

# Run strategy analysis
strategy_analysis = analyze_cost_mitigation_strategies(trainData)
mitigation_strategies = generate_mitigation_strategies(strategy_analysis)
visualize_strategy_analysis(strategy_analysis, trainData)

# Print recommended strategies
print("\nCOST REDUCTION STRATEGIES:")
for strategy in mitigation_strategies['cost_reduction']:
    print(f"\nFactor: {strategy['factor'].split('[')[0]}")
    print(f"Recommended Strategy: {strategy['strategy']}")
    print(f"Potential Impact Score: {strategy['impact_score']:.3f}")

print("\nEFFICIENCY IMPROVEMENT OPPORTUNITIES:")
for strategy in mitigation_strategies['efficiency_improvements']:
    print(f"\nCluster {strategy['cluster']}:")
    print(f"Recommended Strategy: {strategy['strategy']}")
    print(f"Potential Cost Savings: ${strategy['potential_saving']:,.2f}")